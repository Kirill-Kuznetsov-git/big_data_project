{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For EDA part\n",
    "\n",
    "- You need to analyze the data and display at least 5 different insights in the dashboard.\n",
    "- For each insight drawn from the data, you need to create charts and add a description for it.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Non-interactive analysis via spark-submit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "submitting ```model.py```.\n",
    "\n",
    "NOTE:  You should add some jars to properly import the HIVE tables using Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark-submit --jars /usr/hdp/current/hive-client/lib/hive-metastore-1.2.1000.2.6.5.0-292.jar,/usr/hdp/current/hive-client/lib/hive-exec-1.2.1000.2.6.5.0-292.jar --packages org.apache.spark:spark-avro_2.12:3.0.3 scripts/model.py "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Connect to Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"BDT Project\")\\\n",
    "        .config(\"spark.sql.catalogImplementation\",\"hive\")\\\n",
    "        .config(\"hive.metastore.uris\", \"thrift://sandbox-hdp.hortonworks.com:9083\")\\\n",
    "        .config(\"spark.sql.avro.compression.codec\", \"snappy\")\\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. List all databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark.catalog.listDatabases())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. List all tables\n",
    "\n",
    "TODO: specify table name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TABLE_NAME = '?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark.catalog.listTables(TABLE_NAME))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Read Hive table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emps = spark.read.format(\"avro\").table(f'{TABLE_NAME}.employees_part')\n",
    "emps.createOrReplaceTempView('employees')\n",
    "\n",
    "depts = spark.read.format(\"avro\").table(f'{TABLE_NAME}.departments_buck')\n",
    "depts.createOrReplaceTempView('departments')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Run some queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emps.printSchema()\n",
    "depts.printSchema()\n",
    "\n",
    "spark.sql(\"SELECT * FROM employees WHERE deptno=10\").show()\n",
    "\n",
    "spark.sql(\"SELECT * FROM departments\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Interactive analysis via Zeppelin"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a new note and set python2 as the default interpter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "    .appName(\"BDT Project\")\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .config(\"hive.metastore.uris\", \"thrift://sandbox-hdp.hortonworks.com:9083\")\\\n",
    "    .config(\"spark.sql.catalogImplementation\",\"hive\")\\\n",
    "    .config(\"spark.sql.avro.compression.codec\", \"snappy\")\\\n",
    "    .config(\"spark.jars\", \"file:///usr/hdp/current/hive-client/lib/hive-metastore-1.2.1000.2.6.5.0-292.jar,file:///usr/hdp/current/hive-client/lib/hive-exec-1.2.1000.2.6.5.0-292.jar\")\\\n",
    "    .config(\"spark.jars.packages\",\"org.apache.spark:spark-avro_2.12:3.0.3\")\\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(sc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. List all databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark.catalog.listDatabases())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Read Hive table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emps = spark.read.format(\"avro\").table('projectdb.employees_part')\n",
    "emps.createOrReplaceTempView('employees')\n",
    "\n",
    "depts = spark.read.format(\"avro\").table('projectdb.departments_buck')\n",
    "emps.createOrReplaceTempView('departments')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Run some queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emps.printSchema()\n",
    "depts.printSchema()\n",
    "\n",
    "spark.sql(\"SELECT * FROM employees WHERE deptno=10\").show()\n",
    "\n",
    "spark.sql(\"SELECT * FROM departments\").show()\n",
    "\n",
    "spark.sql(\"SELECT AVG(SAL) FROM employees;\").show()\n",
    "spark.sql(\"SELECT * from employees where comm is NULL;\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use pyspark for interactive analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.shape\n",
    "\n",
    "df.columns\n",
    "\n",
    "df.head(10)\n",
    "\n",
    "\n",
    "Filtering out categorical features.\n",
    "\n",
    "- df.dtypes[df.dtypes == 'object']\n",
    "\n",
    "\n",
    "df.isnull().sum()\n",
    "\n",
    "\n",
    "**Feature 1**: \"There are many missing values in ORIGIN_CALL and ORIGIN_STAND because may be all the taxi users have not called the via phone and they have not started their trip from taxi stand. \"\n",
    "\n",
    "df.info()\n",
    "\n",
    "df.describe()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Describing the categorical features.\n",
    "\n",
    "- df.describe(include = ['object'])\n",
    "\n",
    "\n",
    "\n",
    "**Feature 2**: \"We can see the DAY_TYPE has only 1 unique value and that is 'A' which means that all the trips are started on normal day or weekend. \n",
    "\n",
    "Also the 5901 observations don't have the POLYLINE values means we cannot calculate the travel time for those trips.\"\n",
    "\n",
    "\n",
    "\n",
    "df.sort_values('TIMESTAMP',inplace = True)\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n",
    "separate timestamp \n",
    "\n",
    "```python\n",
    "df['year'] = df['TIMESTAMP'].apply(lambda x :datetime.datetime.fromtimestamp(x).year) \n",
    "df['month'] = df['TIMESTAMP'].apply(lambda x :datetime.datetime.fromtimestamp(x).month) \n",
    "df['month_day'] = df['TIMESTAMP'].apply(lambda x :datetime.datetime.fromtimestamp(x).day) \n",
    "df['hour'] = df['TIMESTAMP'].apply(lambda x :datetime.datetime.fromtimestamp(x).hour) \n",
    "df['week_day'] = df['TIMESTAMP'].apply(lambda x :datetime.datetime.fromtimestamp(x).weekday())\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "Pie chart for the year\n",
    "\n",
    "```\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.pie(df['year'].value_counts(), labels = df['year'].value_counts().keys(),autopct = '%.1f%%')\n",
    "```\n",
    "\n",
    "**Feature 3**: \"From the above pie chart it is clear that there are equal number of taxi trips in both the year.\"\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "plt.figure(figsize = (5,5))\n",
    "plt.title('Count of trips per day of week')\n",
    "sns.countplot(y = 'week_day', data = df)\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Day')\n",
    "```\n",
    "\n",
    "**Feature 4**: \"The 4th and 5th day of week has almost same number of trips and rest all the days have almost similar number of trips. This means that we can say each and every day of week required same number of taxies irrespective of weekend or working day.\"\n",
    "\n",
    "\n",
    "```\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.title('Count of trips per month')\n",
    "sns.countplot(y = 'month', data = df)\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Month')\n",
    "```\n",
    "\n",
    "**Feature 5**: \"On an average we can say that every month has atleast 120000 taxi trips planned.\"\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.title('Count of trips per hour')\n",
    "sns.countplot(y = 'hour', data = df)\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Hours')\n",
    "```\n",
    "\n",
    "**Feature 6**: \"14th and 15th hour may be the peak hours, office time, school time because lot of taxies are used between this time.\"\n",
    "\n",
    "\n",
    "\n",
    "drop null polylines\n",
    "\n",
    "convert polyline into total travel time\n",
    "\n",
    "**Feature 7**: \"Above description clear that the minimum travelling time by the taxi is 15 seconds and maximum is 58200 seconds i.e.around 16 hours 16 min.\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 1 - "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 2 - "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 3 - "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 4 - "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 5 - "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
