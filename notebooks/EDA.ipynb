{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For EDA part\n",
    "\n",
    "- You need to analyze the data and display at least 5 different insights in the dashboard.\n",
    "- For each insight drawn from the data, you need to create charts and add a description for it.\n",
    "\n",
    "[10 points] EDA \n",
    "    \n",
    "- provide 5 insights (2 points for each)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Non-interactive analysis via spark-submit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "submitting ```model.py```.\n",
    "\n",
    "NOTE:  You should add some jars to properly import the HIVE tables using Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark-submit --jars /usr/hdp/current/hive-client/lib/hive-metastore-1.2.1000.2.6.5.0-292.jar,/usr/hdp/current/hive-client/lib/hive-exec-1.2.1000.2.6.5.0-292.jar --packages org.apache.spark:spark-avro_2.12:3.0.3 scripts/model.py "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Connect to Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"BDT Project\")\\\n",
    "        .config(\"spark.sql.catalogImplementation\",\"hive\")\\\n",
    "        .config(\"hive.metastore.uris\", \"thrift://sandbox-hdp.hortonworks.com:9083\")\\\n",
    "        .config(\"spark.sql.avro.compression.codec\", \"snappy\")\\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. List all databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark.catalog.listDatabases())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. List all tables\n",
    "\n",
    "TODO: specify table name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TABLE_NAME = '?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark.catalog.listTables(TABLE_NAME))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Read Hive table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emps = spark.read.format(\"avro\").table(f'{TABLE_NAME}.employees_part')\n",
    "emps.createOrReplaceTempView('employees')\n",
    "\n",
    "depts = spark.read.format(\"avro\").table(f'{TABLE_NAME}.departments_buck')\n",
    "depts.createOrReplaceTempView('departments')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Run some queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emps.printSchema()\n",
    "depts.printSchema()\n",
    "\n",
    "spark.sql(\"SELECT * FROM employees WHERE deptno=10\").show()\n",
    "\n",
    "spark.sql(\"SELECT * FROM departments\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Interactive analysis via Zeppelin"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a new note and set python2 as the default interpter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "    .appName(\"BDT Project\")\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .config(\"hive.metastore.uris\", \"thrift://sandbox-hdp.hortonworks.com:9083\")\\\n",
    "    .config(\"spark.sql.catalogImplementation\",\"hive\")\\\n",
    "    .config(\"spark.sql.avro.compression.codec\", \"snappy\")\\\n",
    "    .config(\"spark.jars\", \"file:///usr/hdp/current/hive-client/lib/hive-metastore-1.2.1000.2.6.5.0-292.jar,file:///usr/hdp/current/hive-client/lib/hive-exec-1.2.1000.2.6.5.0-292.jar\")\\\n",
    "    .config(\"spark.jars.packages\",\"org.apache.spark:spark-avro_2.12:3.0.3\")\\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(sc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. List all databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark.catalog.listDatabases())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Read Hive table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emps = spark.read.format(\"avro\").table('projectdb.employees_part')\n",
    "emps.createOrReplaceTempView('employees')\n",
    "\n",
    "depts = spark.read.format(\"avro\").table('projectdb.departments_buck')\n",
    "emps.createOrReplaceTempView('departments')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Run some queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emps.printSchema()\n",
    "depts.printSchema()\n",
    "\n",
    "spark.sql(\"SELECT * FROM employees WHERE deptno=10\").show()\n",
    "\n",
    "spark.sql(\"SELECT * FROM departments\").show()\n",
    "\n",
    "spark.sql(\"SELECT AVG(SAL) FROM employees;\").show()\n",
    "spark.sql(\"SELECT * from employees where comm is NULL;\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use pyspark for interactive analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: take the code from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"avro\").table(\"my_database.trips\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insight 1 - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the shape of the DataFrame\n",
    "print((df.count(), len(df.columns)))\n",
    "\n",
    "# get the column names of the DataFrame\n",
    "print(df.columns)\n",
    "\n",
    "# show the first 10 rows of the DataFrame\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out categorical features\n",
    "\n",
    "cat_cols = [field.name for field in df.schema.fields if isinstance(\n",
    "    field.dataType, StringType) or isinstance(field.dataType, LongType)]\n",
    "\n",
    "print(\"Result: \", cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of missing values in each column of the DataFrame\n",
    "missing_vals = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "\n",
    "# display the results\n",
    "missing_vals.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insight 1**: \"There are many missing values in ORIGIN_CALL and ORIGIN_STAND because may be all the taxi users have not called the via phone and they have not started their trip from taxi stand\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insight 2 - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the schema of the DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "# compute summary statistics for all numeric columns\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# compute summary statistics for all categorical columns\n",
    "df.select([col(c).describe() for c in df.columns if c in cat_cols]).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insight 2**: \"We can see the DAY_TYPE has only 1 unique value and that is 'A' which means that all the trips are started on normal day or weekend. \n",
    "\n",
    "Also the 5901 observations don't have the POLYLINE values means we cannot calculate the travel time for those trips.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insight 3 - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import asc\n",
    "\n",
    "# sort the DataFrame by the timestamp column in ascending order\n",
    "df_sorted = df.orderBy('timestamp_col', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format, to_date, dayofweek\n",
    "\n",
    "# add new columns for year, month, day, hour, and day of the week\n",
    "df = df.withColumn('year', date_format('timestamp_col', 'y')) \\\n",
    "    .withColumn('month', date_format('timestamp_col', 'M')) \\\n",
    "    .withColumn('day', date_format('timestamp_col', 'd')) \\\n",
    "    .withColumn('hour', date_format('timestamp_col', 'H')) \\\n",
    "    .withColumn('day_of_week', dayofweek(to_date('timestamp_col'))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie chart for the year\n",
    "\n",
    "# TODO: rewrite this for zeppelin\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.pie(df['year'].value_counts(), labels = df['year'].value_counts().keys(),autopct = '%.1f%%')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insight 3**: \"From the above pie chart it is clear that there are equal number of taxi trips in both the year.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insight 4 - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chart of trips per day of week\n",
    "\n",
    "# TODO: rewrite this for zeppelin\n",
    "plt.figure(figsize = (5,5))\n",
    "plt.title('Count of trips per day of week')\n",
    "sns.countplot(y = 'week_day', data = df)\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Day')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insight 4**: \"The 4th and 5th day of week has almost same number of trips and rest all the days have almost similar number of trips. This means that we can say each and every day of week required same number of taxies irrespective of weekend or working day.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insight 5 - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chart of trips per month\n",
    "\n",
    "# TODO: rewrite this for zeppelin\n",
    "\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.title('Count of trips per month')\n",
    "sns.countplot(y = 'month', data = df)\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Month')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insight 5**: \"On an average we can say that every month has atleast 120000 taxi trips planned.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insight 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chart of trips per hour\n",
    "\n",
    "# TODO: rewrite this for zeppelin\n",
    "\n",
    "\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.title('Count of trips per hour')\n",
    "sns.countplot(y = 'hour', data = df)\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Hours')\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insight 6**: \"14th and 15th hour may be the peak hours, office time, school time because lot of taxies are used between this time.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insight 7"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drop null polylines\n",
    "\n",
    "convert polyline into total travel time\n",
    "\n",
    "**Insight 7**: \"Above description clear that the minimum travelling time by the taxi is 15 seconds and maximum is 58200 seconds i.e.around 16 hours 16 min.\"\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
